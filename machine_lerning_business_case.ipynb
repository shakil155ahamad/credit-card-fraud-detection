{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "raw_csv_data=np.loadtxt('Audiobooks_data.csv',delimiter=',')\n",
    "\n",
    "unscaled_inputs_all=raw_csv_data[:,1:-1]\n",
    "\n",
    "targets_all=raw_csv_data[:,-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices=np.arange(unscaled_inputs_all.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "unscaled_inputs_all=unscaled_inputs_all[shuffled_indices]\n",
    "\n",
    "targets_all=targets_all[shuffled_indices]\n",
    "                        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balencing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets=int(np.sum(targets_all))\n",
    "\n",
    "indices_to_remove=[]\n",
    "\n",
    "zero_target_counter=0\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i]==0:\n",
    "        zero_target_counter +=1\n",
    "        if zero_target_counter > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "unscaled_inputs_equal_priors=np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
    " \n",
    "targets_all_equal_priors=np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standardizing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs=preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices=np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_scaled_inputs=scaled_inputs[shuffled_indices]\n",
    "\n",
    "shuffled_targets=targets_all_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into train validation and tets datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4474\n",
      "1794.0\n",
      "219.0\n",
      "224.0\n"
     ]
    }
   ],
   "source": [
    "sample_count=shuffled_scaled_inputs.shape[0]\n",
    "\n",
    "train_sample_count=int(0.80*sample_count)\n",
    "\n",
    "validation_sample_count=int(0.1*sample_count)\n",
    "\n",
    "test_sample_count=sample_count-train_sample_count-validation_sample_count\n",
    "\n",
    "train_input=shuffled_scaled_inputs[:train_sample_count]\n",
    "train_target=shuffled_targets[:train_sample_count]\n",
    "\n",
    "validation_input=shuffled_scaled_inputs[train_sample_count:train_sample_count+validation_sample_count]\n",
    "validation_target=shuffled_targets[train_sample_count:train_sample_count+validation_sample_count]\n",
    "\n",
    "test_input=shuffled_scaled_inputs[train_sample_count+validation_sample_count:]\n",
    "test_target=shuffled_targets[train_sample_count+validation_sample_count:]\n",
    "\n",
    "print(sample_count)\n",
    "print(np.sum(train_target))\n",
    "print(np.sum(validation_target))\n",
    "print(np.sum(test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.94761239, -0.853997  ,  0.61381346, ..., -0.43427123,\n",
       "        -0.1456124 ,  0.34837297],\n",
       "       [-0.94761239, -0.853997  ,  0.17558209, ..., -0.43427123,\n",
       "        -0.1456124 , -0.68528793],\n",
       "       [ 0.13304507, -0.23782125, -0.36913539, ...,  1.88472077,\n",
       "        -0.1456124 , -0.76065903],\n",
       "       ...,\n",
       "       [-2.56859858, -1.47017274, -0.36913539, ..., -0.43427123,\n",
       "        -0.1456124 , -0.76065903],\n",
       "       [-1.81213836, -1.34693759, -0.36913539, ..., -0.43427123,\n",
       "        -0.1456124 ,  1.44663767],\n",
       "       [ 0.13304507, -0.23782125, -0.36913539, ..., -0.43427123,\n",
       "        -0.1456124 , -0.30843239]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save datasts into npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobook_data_train',inputs=train_input,targets=train_target)\n",
    "np.savez('Audiobook_data_validation',inputs=validation_input,targets=validation_target)\n",
    "np.savez('Audiobook_data_test',inputs=test_input,targets=test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3579\n",
      "3579\n"
     ]
    }
   ],
   "source": [
    "npz1=np.load('Audiobook_data_train.npz')\n",
    "train_inputs=npz1['inputs'].astype(np.float)\n",
    "train_targets=npz1['targets'].astype(np.int)\n",
    "\n",
    "npz2=np.load('Audiobook_data_validation.npz')\n",
    "validation_inputs=npz2['inputs'].astype(np.float)\n",
    "validation_targets=npz2['targets'].astype(np.int)\n",
    "\n",
    "npz3=np.load('Audiobook_data_test.npz')\n",
    "test_inputs=npz3['inputs'].astype(np.float)\n",
    "test_targets=npz3['targets'].astype(np.int)\n",
    "\n",
    "print(len(train_inputs))\n",
    "print(len(train_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 3s - loss: 0.7390 - accuracy: 0.5136 - val_loss: 0.6860 - val_accuracy: 0.5213\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.6787 - accuracy: 0.5546 - val_loss: 0.6539 - val_accuracy: 0.5615\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.6517 - accuracy: 0.6099 - val_loss: 0.6274 - val_accuracy: 0.6421\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.6245 - accuracy: 0.6695 - val_loss: 0.5990 - val_accuracy: 0.6957\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.5966 - accuracy: 0.7016 - val_loss: 0.5707 - val_accuracy: 0.7226\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.5688 - accuracy: 0.7251 - val_loss: 0.5430 - val_accuracy: 0.7248\n",
      "Epoch 7/100\n",
      "3579/3579 - 0s - loss: 0.5428 - accuracy: 0.7332 - val_loss: 0.5195 - val_accuracy: 0.7293\n",
      "Epoch 8/100\n",
      "3579/3579 - 0s - loss: 0.5206 - accuracy: 0.7413 - val_loss: 0.4983 - val_accuracy: 0.7383\n",
      "Epoch 9/100\n",
      "3579/3579 - 0s - loss: 0.5012 - accuracy: 0.7471 - val_loss: 0.4815 - val_accuracy: 0.7450\n",
      "Epoch 10/100\n",
      "3579/3579 - 0s - loss: 0.4845 - accuracy: 0.7544 - val_loss: 0.4672 - val_accuracy: 0.7517\n",
      "Epoch 11/100\n",
      "3579/3579 - 0s - loss: 0.4700 - accuracy: 0.7580 - val_loss: 0.4544 - val_accuracy: 0.7494\n",
      "Epoch 12/100\n",
      "3579/3579 - 0s - loss: 0.4592 - accuracy: 0.7600 - val_loss: 0.4438 - val_accuracy: 0.7427\n",
      "Epoch 13/100\n",
      "3579/3579 - 0s - loss: 0.4492 - accuracy: 0.7645 - val_loss: 0.4362 - val_accuracy: 0.7517\n",
      "Epoch 14/100\n",
      "3579/3579 - 0s - loss: 0.4409 - accuracy: 0.7597 - val_loss: 0.4274 - val_accuracy: 0.7472\n",
      "Epoch 15/100\n",
      "3579/3579 - 0s - loss: 0.4337 - accuracy: 0.7689 - val_loss: 0.4208 - val_accuracy: 0.7472\n",
      "Epoch 16/100\n",
      "3579/3579 - 0s - loss: 0.4279 - accuracy: 0.7740 - val_loss: 0.4150 - val_accuracy: 0.7629\n",
      "Epoch 17/100\n",
      "3579/3579 - 0s - loss: 0.4219 - accuracy: 0.7734 - val_loss: 0.4092 - val_accuracy: 0.7606\n",
      "Epoch 18/100\n",
      "3579/3579 - 0s - loss: 0.4173 - accuracy: 0.7751 - val_loss: 0.4043 - val_accuracy: 0.7673\n",
      "Epoch 19/100\n",
      "3579/3579 - 0s - loss: 0.4139 - accuracy: 0.7756 - val_loss: 0.4007 - val_accuracy: 0.7651\n",
      "Epoch 20/100\n",
      "3579/3579 - 0s - loss: 0.4098 - accuracy: 0.7790 - val_loss: 0.3971 - val_accuracy: 0.7606\n",
      "Epoch 21/100\n",
      "3579/3579 - 0s - loss: 0.4060 - accuracy: 0.7756 - val_loss: 0.3948 - val_accuracy: 0.7696\n",
      "Epoch 22/100\n",
      "3579/3579 - 0s - loss: 0.4029 - accuracy: 0.7809 - val_loss: 0.3911 - val_accuracy: 0.7606\n",
      "Epoch 23/100\n",
      "3579/3579 - 0s - loss: 0.4000 - accuracy: 0.7784 - val_loss: 0.3889 - val_accuracy: 0.7673\n",
      "Epoch 24/100\n",
      "3579/3579 - 0s - loss: 0.3972 - accuracy: 0.7829 - val_loss: 0.3871 - val_accuracy: 0.7651\n",
      "Epoch 25/100\n",
      "3579/3579 - 0s - loss: 0.3948 - accuracy: 0.7826 - val_loss: 0.3857 - val_accuracy: 0.7673\n",
      "Epoch 26/100\n",
      "3579/3579 - 0s - loss: 0.3922 - accuracy: 0.7843 - val_loss: 0.3822 - val_accuracy: 0.7673\n",
      "Epoch 27/100\n",
      "3579/3579 - 0s - loss: 0.3906 - accuracy: 0.7843 - val_loss: 0.3837 - val_accuracy: 0.7718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de68736748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size=10\n",
    "output_size=2\n",
    "hidden_layers_size=10\n",
    "\n",
    "model=tf.keras.Sequential([tf.keras.layers.Dense(hidden_layers_size,activation='relu'),\n",
    "                           tf.keras.layers.Dense(hidden_layers_size,activation='relu'),\n",
    "                           tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "                          ])\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "batch_size=150\n",
    "max_epochs=100\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping(patience=1)\n",
    " \n",
    "model.fit(train_inputs,train_targets,batch_size=batch_size,callbacks=[early_stopping],epochs=max_epochs,validation_data=(validation_inputs,validation_targets),verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
